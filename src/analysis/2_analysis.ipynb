{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A geographical analysis of anatomy donor registers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import json \n",
    "import plotly.express as px\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomy donors dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors = pd.read_csv(\"out/donors.csv\", index_col=\"id\")\n",
    "print(donors.shape)\n",
    "donors.head()\n",
    "donors.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors[\"death_date\"] = pd.to_datetime(donors[\"death_date\"])\n",
    "donors[\"reception_date\"] = pd.to_datetime(donors[\"reception_date\"])\n",
    "donors[\"return_date_sepulture\"] = pd.to_datetime(donors[\"return_date_sepulture\"])\n",
    "donors[\"burial_date\"] = pd.to_datetime(donors[\"burial_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographical dataset of suburbs in NSW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset name:** _NSW Local Government Areas - Geoscape Administrative Boundaries_\n",
    "\n",
    "This dataset was originally found on data.gov.au \"NSW Suburb/Locality Boundaries - Geoscape Administrative Boundaries\". Please visit the source to access the original metadata of the dataset:\n",
    "https://data.gov.au/data/dataset/91e70237-d9d1-4719-a82f-e71b811154c6\n",
    "\n",
    "**Attribution:** Incorporates or developed using Administrative Boundaries © Geoscape Australia licensed by the Commonwealth of Australia under Creative Commons Attribution 4.0 International licence (CC BY 4.0).\n",
    "\n",
    "**Last updated:** 27/02/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suburbs = gpd.read_file(\"data/nsw_localities/nsw_localities.shp\")\n",
    "suburbs = suburbs[[\"LOC_NAME\", \"geometry\"]]\n",
    "suburbs.columns = [\"suburb\", \"geometry\"]\n",
    "print(suburbs.shape)\n",
    "suburbs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_subs = suburbs[suburbs.duplicated([\"suburb\"], keep=False)]\n",
    "print(dup_subs.shape)\n",
    "dup_subs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 4615 localities in NSW, there are over 200 localities in NSW with the same name! This could present a large issue for us when we extract suburb information from the anatomy dataset and attempt to assign it a `geometry` using this dataset: our anatomy records could be assigned to a suburb with the same name in a completely different location.\n",
    "\n",
    "Unfortunately, there is not much we can do about this currently. The reality is, bodies in the anatomy registers came from _all over New South Wales_, and so reducing the dataset in any way – such as by choosing the duplicate suburb closest to Sydney – may not be a valid step in the data cleaning process. We will just have to keep this issue of duplicates in mind when we perform the merge between our anatomy data and our localities data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting `suburb` from donor data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to run a very simple classification algorithm on the entries in our donor dataset to try and extract the suburb of where the patient passed away. \n",
    "- For each row in the donor dataset we check if `death_place` contains _any_ of the suburbs listed in the NSW Government dataset from above.\n",
    "- If so, we extract **all** of the matches (as one entry could list more than one suburb in it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suburbs_ls = suburbs[\"suburb\"].str.lower().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regex and `extractall()` to extract suburb names from place of death\n",
    "regex = r'\\b(' + '|'.join(suburbs_ls) + r')\\b'\n",
    "matches = donors[\"death_place\"].str.extractall(regex)\n",
    "matches = matches.reset_index()\n",
    "matches.columns = [\"id\", \"match\", \"suburb\"]\n",
    "\n",
    "# Show a part of the dataset where one entry has multiple suburb classifications\n",
    "matches[3773:3781][[\"id\", \"match\", \"suburb\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above shows the entry `id`, the `match` number, and the classified `suburb`. If an entry was found to contain more than one suburb, one `id` would have 2 or 3 corresponding `match` rows. Let's make this easier to see using a pivot table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches[\"suburb_count\"] = matches.groupby('id').cumcount() + 1\n",
    "matches = matches.pivot_table(index='id', columns='suburb_count', values='suburb', aggfunc='first').rename(columns=lambda x: f'suburb_{x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches[matches[\"suburb_2\"].notna()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a pivot table with suburb extractions, we can perform some basic cleaning before joining it back to the master dataset. For example, if all rows `suburb_1`, `suburb_2` and `suburb_3` contain the same suburb, we can just keep `suburb_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(row: pd.Series):\n",
    "    if row[\"suburb_1\"] == row[\"suburb_2\"]:\n",
    "        return True\n",
    "    \n",
    "    if row[\"suburb_2\"] == row[\"suburb_3\"] and pd.notna(row[\"suburb_2\"]):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Clean duplicate entries\n",
    "matches.loc[matches.apply(func=find_duplicates, axis=1), \"suburb_2\":] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now joining the two tables together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join pivot table on original donors dataset\n",
    "donors_suburb = donors.join(matches, on=\"id\", how=\"left\")\n",
    "donors_suburb.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking how many entries still contain multiple classifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors_suburb[donors_suburb[\"suburb_2\"].notna() | donors_suburb[\"suburb_3\"].notna()][[\"death_place\", \"suburb_1\", \"suburb_2\", \"suburb_3\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the table above, there are still many rows where multiple suburbs have been extracted from the `death_place`.\n",
    "\n",
    "One of the reasons for this is because street names often include the name of a different suburb e.g. `160 liverpool road enfield` contains two suburb names due to *Liverpool* Road running through *Enfield*.\n",
    "\n",
    "Let's try and fix these entries. As the suburb usually comes after the street name in addresses, we will extract the last found suburb from entries containing an address. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_keywords = [\"avenue\", \"av\", \"ave\", \"boulevard\", \"bvd\", \"chase\", \"ch\", \"circuit\", \"cct\", \"close\", \"cl\", \"court\", \"ct\", \"crescent\", \"cr\", \"cres\", \"crest\", \"crst\", \"drive\", \"dr\", \"esplanade\", \"esp\", \"grange\", \"gra\", \"grove\", \"gr\", \"highway\", \"hwy\", \"lane\", \"la\", \"mall\", \"parade\", \"pde\", \"parkway\", \"pwy\", \"place\", \"pl\", \"road\", \"rd\", \"square\", \"sq\", \"street\", \"st\", \"terrace\", \"tce\", \"way\", \"walk\"]\n",
    "address_pattern = fr\"\\b(?:{'|'.join(address_keywords)})\\b\"\n",
    "\n",
    "def find_and_clean_extra_suburbs(row: pd.Series):\n",
    "    if ~pd.Series(row[\"death_place\"]).str.contains(address_pattern, na=False).any():\n",
    "        return row\n",
    "\n",
    "    if pd.notna(row[\"suburb_3\"]):\n",
    "        row[\"suburb_1\"] = row[\"suburb_3\"]\n",
    "        row[\"suburb_2\"] = np.nan\n",
    "        row[\"suburb_3\"] = np.nan\n",
    "        return row\n",
    "    if pd.notna(row[\"suburb_2\"]):\n",
    "        row[\"suburb_1\"] = row[\"suburb_2\"]\n",
    "        row[\"suburb_2\"] = np.nan\n",
    "        return row\n",
    "    return row\n",
    "\n",
    "donors_suburb = donors_suburb.apply(func=find_and_clean_extra_suburbs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many entries with multiple suburb classfications still exist after completing the above cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_suburbs = donors_suburb[donors_suburb[\"suburb_2\"].notna() | donors_suburb[\"suburb_3\"].notna()][[\"death_place\", \"suburb_1\", \"suburb_2\", \"suburb_3\"]]\n",
    "indices_to_fix = multiple_suburbs.index\n",
    "multiple_suburbs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing ambiguous suburb data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about 100 or so entries which remain with 2 or more suburb classifications. Let's manually go through these and fix them up ourselves.\n",
    "\n",
    "The way we do this is by first creating a data entry portal using `ipywidgets` – this way, all the data entry can be done inside this notebook, with the storage of the manually labelled suburbs taken care of by the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = widgets.Output()\n",
    "text_input = widgets.Text(\n",
    "    description='Suburb:',\n",
    "    placeholder='Type correct suburb here...'\n",
    ")\n",
    "submit_button = widgets.Button(description=\"Submit\")\n",
    "display(text_input, submit_button, output)\n",
    "\n",
    "i = 0\n",
    "correction_code = []\n",
    "def process_entry():\n",
    "    global i\n",
    "    with output:\n",
    "        clear_output()\n",
    "        if i < len(indices_to_fix):\n",
    "            index_to_fix = indices_to_fix[i]\n",
    "\n",
    "            # Save the input correction to the DataFrame\n",
    "            if text_input.value:  # Check if there is any input\n",
    "                # Assign the new value and print the code for this correction\n",
    "                donors_suburb.loc[index_to_fix, \"suburb_1\"] = text_input.value\n",
    "                print(f\"Corrected entry:\\n{donors_suburb.loc[index_to_fix]}\\n\")\n",
    "                \n",
    "                correction_code.append(f\"donors_suburb.loc[{index_to_fix}, 'suburb_1'] = '{text_input.value}'\")\n",
    "\n",
    "            # Move to the next index\n",
    "            i += 1\n",
    "\n",
    "            if i < len(indices_to_fix):\n",
    "                # Display the next item to correct\n",
    "                index_to_fix = indices_to_fix[i]\n",
    "                print(f\"Next to correct: {donors_suburb.loc[index_to_fix, 'death_place']}\")\n",
    "                if pd.notna(donors_suburb.loc[index_to_fix, \"suburb_1\"]):\n",
    "                    print(f\"Pre-identified as 1. {donors_suburb.loc[index_to_fix, 'suburb_1']}\")\n",
    "                if pd.notna(donors_suburb.loc[index_to_fix, \"suburb_2\"]):\n",
    "                    print(f\"Pre-identified as 2. {donors_suburb.loc[index_to_fix, 'suburb_2']}\")\n",
    "                if pd.notna(donors_suburb.loc[index_to_fix, \"suburb_3\"]):\n",
    "                    print(f\"Pre-identified as 3. {donors_suburb.loc[index_to_fix, 'suburb_3']}\")\n",
    "\n",
    "                text_input.value = '' \n",
    "            else:\n",
    "                print(\"All entries have been processed\")\n",
    "        else:\n",
    "            print(\"All entries have been processed\")\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    process_entry()\n",
    "\n",
    "def on_text_submit(change):\n",
    "    process_entry()\n",
    "\n",
    "submit_button.on_click(on_button_clicked)\n",
    "text_input.on_submit(on_text_submit)\n",
    "\n",
    "# Display the first entry to correct when the cell is run\n",
    "with output:\n",
    "    if ~indices_to_fix.empty: \n",
    "        initial_index_to_fix = indices_to_fix[0]\n",
    "        print(f\"First to correct: {donors_suburb.loc[initial_index_to_fix, 'death_place']}\")\n",
    "    else:\n",
    "        print(\"No entries to fix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above program and entering in the correct suburb information manually, the data entry portal outputs a list of the code used to update the dataset - this way we do not have to re-enter all 100 entries everytime this notebook is run. You can see `out/fix_suburb_code.txt` for the full list of correction code.\n",
    "\n",
    "We will now execute those lines to make sure our dataset is up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out/fix_suburb_code.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        exec(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now confidently select `suburb_1` for all our entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors_suburb = donors_suburb.drop(columns=[\"suburb_2\", \"suburb_3\"])\n",
    "donors_suburb = donors_suburb.rename(columns={\"suburb_1\": \"suburb\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors_suburb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling in missing suburb information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_suburb = donors_suburb[donors_suburb[\"suburb\"].isna()]\n",
    "missing_suburb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still about 500 entries which are missing a suburb classification. This may be due to spelling errors or the total absence of a suburb name in the place of death. Let's use a similar method as before to fill in these missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deathplace_suburb = {}\n",
    "deathplace_missing_suburb = missing_suburb[\"death_place\"].unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = widgets.Output()\n",
    "text_input = widgets.Text(\n",
    "    description='Suburb:',\n",
    "    placeholder='Type correct suburb here...'\n",
    ")\n",
    "submit_button = widgets.Button(description=\"Submit\")\n",
    "display(text_input, submit_button, output)\n",
    "\n",
    "i = 0\n",
    "def process_entry():\n",
    "    global i\n",
    "    with output:\n",
    "        clear_output()\n",
    "        if i < len(deathplace_missing_suburb):\n",
    "            death_place = deathplace_missing_suburb[i]\n",
    "\n",
    "            # Save the input correction to the dictionary\n",
    "            if text_input.value: \n",
    "                deathplace_suburb[death_place] = text_input.value\n",
    "                print(f\"'{death_place}' --> '{text_input.value}'\\n\")\n",
    "\n",
    "            i += 1\n",
    "\n",
    "            if i < len(deathplace_missing_suburb):\n",
    "                print(f\"Next to correct: {deathplace_missing_suburb[i]}\")\n",
    "                text_input.value = ''\n",
    "            else:\n",
    "                print(\"All entries have been processed\")\n",
    "        else:\n",
    "            print(\"All entries have been processed\")\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    process_entry()\n",
    "\n",
    "def on_text_submit(change):\n",
    "    process_entry()\n",
    "\n",
    "submit_button.on_click(on_button_clicked)\n",
    "text_input.on_submit(on_text_submit)\n",
    "\n",
    "# Display the first entry to correct when the cell is run\n",
    "with output:\n",
    "    if deathplace_missing_suburb:\n",
    "        print(f\"First to correct: {deathplace_missing_suburb[0]}\")\n",
    "    else:\n",
    "        print(\"No entries to fix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to above, I have used the data entry interface above to go through and manually label the suburb for the unique places of death which were missing them. These have been saved in `out/fill_suburb_dict.txt`.\n",
    "\n",
    "All we have to do now is to read in the dictionary and update the suburbs for the relevant entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out/fill_suburb_dict.txt\", \"r\") as f:\n",
    "    deathplace_suburb = json.load(f)\n",
    "\n",
    "donors_suburb['suburb'] = donors_suburb['death_place'].map(deathplace_suburb).fillna(donors_suburb['suburb'])\n",
    "\n",
    "# Optionally, filter out the entries that map to \"NA\" to avoid replacing them\n",
    "for place, suburb in deathplace_suburb.items():\n",
    "    if suburb == \"NA\":\n",
    "        donors_suburb.loc[donors_suburb['death_place'] == place, 'suburb'] = pd.NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking how many missing entries remain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_suburb = donors_suburb[donors_suburb[\"suburb\"].isna()]\n",
    "print(missing_suburb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_suburb_by_cat = missing_suburb.groupby([\"death_place_category\"], as_index=False).size()\n",
    "missing_suburb_by_cat.columns = [\"death_place_category\", \"count\"]\n",
    "\n",
    "# Create the pie chart with labels outside the chart\n",
    "fig = px.pie(missing_suburb_by_cat, values='count', names='death_place_category',\n",
    "             labels={\"death_place_category\": \"Place of death category\"},\n",
    "             hole=0.3)\n",
    "\n",
    "# Update traces to position text outside and show lines\n",
    "fig.update_traces(\n",
    "    textposition='auto',\n",
    "    textinfo='percent+label',\n",
    "    pull=[0.1 for i in range(len(missing_suburb_by_cat))],\n",
    "    showlegend=False,\n",
    "    automargin=True\n",
    ")\n",
    "\n",
    "# Update the layout to adjust the margins\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, t=30, b=30)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# with open(\"out/missing_suburbs.pdf\", \"wb\") as f:\n",
    "#     f.write(fig.to_image(format=\"pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have gone from over 470 missing entries to 86. About 2/3 of the remaining 86 entries can be attributed to deaths at unspecified morgues, and a large majority of the remaining 1/3 entries are missing place of death values as they were neonatal deaths or still births."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the geographical distribution of place of death"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging `donors` and `localities`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to merge our two datasets together. However, as we noted at the start, the `suburb` column in the `localities` dataset is NOT unique. There are multiple localities in NSW with the same name, but different locations. Keeping this in mind, let's merge the two datasets and see whether there are any such duplicate suburbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to title case before merge\n",
    "donors_suburb[\"suburb\"] = donors_suburb[\"suburb\"].str.title()\n",
    "\n",
    "# Excluding entries with NA values for suburb\n",
    "donors_suburb = donors_suburb.dropna(subset=[\"suburb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on donors_suburb to maintain duplicates\n",
    "donors_geo = gpd.GeoDataFrame(pd.merge(donors_suburb, suburbs, on=\"suburb\", how=\"left\"), crs=\"EPSG:7844\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all duplicates\n",
    "dup_rows = donors_geo.groupby(donors_geo.columns.difference(['geometry']).tolist()).filter(lambda x: x['geometry'].nunique() > 1)\n",
    "\n",
    "# Get all duplicate suburbs\n",
    "dup_subs = dup_rows[\"suburb\"].unique()\n",
    "print(dup_subs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above 5 suburbs are all duplicated in our dataset. Let's see where these suburbs are on the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth_mapbox(dup_rows,\n",
    "                    geojson=dup_rows.geometry,\n",
    "                    color=dup_rows.index,\n",
    "                    locations=dup_rows.index,\n",
    "                    center={\"lat\": -33.2371, \"lon\": 151.5623},\n",
    "                    zoom=7,\n",
    "                    mapbox_style=\"open-street-map\",\n",
    "                    hover_name=\"suburb\",\n",
    "                    opacity=0.6\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Death Counts by Suburb (w/o outliers)\",\n",
    "    margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring the map, it is clear what is going on. Summer Hill, Punchbowl, Enmore, and Darlington are all inner city suburbs which happen to have rural counterparts! We can be sure that the entries are not from the rural suburbs by glancing over their `death_place` entries – they are all addresses of Sydney. Finally, Budgewoi is a coastal town which is naturally divided into two by the Budgewoi Lake. This is not a concern for us.\n",
    "\n",
    "So, we will choose the entries for Summer Hill, Punchbowl, Enmore, and Darlington, which are closest to the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "sydney_centre = Point(151.2093, -33.8688)\n",
    "donors_geo[\"dist_to_sydney\"] = donors_geo[\"geometry\"].distance(sydney_centre)\n",
    "\n",
    "donors_geo[\"closest\"] = True\n",
    "\n",
    "city_suburbs = [\"Summer Hill\", \"Punchbowl\", \"Enmore\", \"Darlington\"]\n",
    "furthest_entries = donors_geo[donors_geo[\"suburb\"].isin(city_suburbs)] \\\n",
    "                        .sort_values(by=\"dist_to_sydney\", ascending=False) \\\n",
    "                        .drop_duplicates(\n",
    "                            subset=donors_geo.columns.difference([\"geometry\", \"dist_to_sydney\"]),\n",
    "                            keep=\"first\"\n",
    "                        )\n",
    "donors_geo.loc[furthest_entries.index, \"closest\"] = False\n",
    "donors_geo = donors_geo[donors_geo[\"closest\"] == True]\n",
    "donors_geo = donors_geo.sort_index()\n",
    "donors_geo = donors_geo.drop(columns=[\"dist_to_sydney\", \"closest\"])\n",
    "donors_geo[\"geometry\"] = donors_geo[\"geometry\"].simplify(tolerance=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suburb_counts = donors_geo.groupby([\"suburb\", \"geometry\"]).size().reset_index(name=\"count\")\n",
    "suburb_counts = gpd.GeoDataFrame(suburb_counts, geometry=suburb_counts[\"geometry\"])\n",
    "suburb_counts = suburb_counts.set_index(\"suburb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a logarithmic transformation to the 'count' column to reduce the impact of outliers\n",
    "suburb_counts[\"log_count\"] = np.log1p(suburb_counts[\"count\"]) \n",
    "\n",
    "fig = px.choropleth_mapbox(\n",
    "    suburb_counts, \n",
    "    geojson=suburb_counts.geometry,\n",
    "    locations=suburb_counts.index, \n",
    "    color=\"log_count\",\n",
    "    color_continuous_scale=\"OrRd\",\n",
    "    hover_name=suburb_counts.index,\n",
    "    hover_data={\"log_count\": False, \"count\": True},\n",
    "    labels={\"log_count\": \"Logarithmic death count\", \"count\": \"Death count\"},\n",
    "    mapbox_style=\"open-street-map\",\n",
    "    center={\"lat\": -33.883, \"lon\": 151.206},\n",
    "    zoom=8,\n",
    "    opacity=0.8\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Death Counts by Suburb\",\n",
    "    margin={\"r\":10,\"t\":70,\"l\":20,\"b\":20}\n",
    ")\n",
    "fig.show()\n",
    "# fig.write_html(\"out/choropleth_counts.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_cat = donors_geo.groupby([\"suburb\", \"geometry\"])[\"death_place_category\"].agg(lambda x: pd.Series.mode(x)[0] if not x.empty else None).reset_index() # Pick the first mode\n",
    "most_common_cat = gpd.GeoDataFrame(most_common_cat, geometry=most_common_cat[\"geometry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth_mapbox(\n",
    "    most_common_cat, \n",
    "    geojson=most_common_cat.geometry,\n",
    "    locations=most_common_cat.index, \n",
    "    color=\"death_place_category\",\n",
    "    color_discrete_sequence=px.colors.qualitative.D3,\n",
    "    hover_name=\"suburb\",\n",
    "    hover_data={\"death_place_category\": True},\n",
    "    labels={\"death_place_category\": \"Category\"},\n",
    "    mapbox_style=\"open-street-map\",\n",
    "    center={\"lat\": -33.883, \"lon\": 151.206},\n",
    "    zoom=8,\n",
    "    opacity=0.75\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Suburbs by their most common place of death category\",\n",
    "    margin={\"r\":10,\"t\":70,\"l\":20,\"b\":20}\n",
    ")\n",
    "# fig.write_html(\"out/choropleth_categories.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors_binned = donors_geo\n",
    "donors_binned[\"decade\"] = (donors_binned[\"death_date\"].dt.year // 10) * 10\n",
    "grouped = donors_geo.groupby([\"decade\", \"suburb\", \"geometry\"])[\"death_place_category\"].agg(category=(lambda x: x.mode()[0]), count=(lambda x: x.count())).reset_index()\n",
    "cat_by_decade = gpd.GeoDataFrame(grouped, geometry=grouped[\"geometry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth_mapbox(\n",
    "    cat_by_decade,\n",
    "    geojson=cat_by_decade.geometry,\n",
    "    locations=cat_by_decade.index,  \n",
    "    color=\"category\",\n",
    "    color_discrete_sequence=px.colors.qualitative.D3,\n",
    "    hover_name=\"suburb\",\n",
    "    hover_data={\"count\": True, \"decade\": False},\n",
    "    mapbox_style=\"open-street-map\",\n",
    "    center={\"lat\": -33.883, \"lon\": 151.206},\n",
    "    zoom=8,\n",
    "    opacity=0.75,\n",
    "    animation_frame=\"decade\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Suburbs and their most common death place over time\",\n",
    "    margin={\"r\":10,\"t\":70,\"l\":20,\"b\":20}\n",
    ")\n",
    "\n",
    "# fig.write_html(\"out/choropleth_time.html\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
